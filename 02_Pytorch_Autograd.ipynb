{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMP00w/qpqMmxeqNqV+s0r6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilanjan1210/PyTorch-Fundamentals/blob/main/02_Pytorch_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Autograd\n",
        "\n",
        "PyTorch Autograd is the automatic differentiation engine that powers neural network training within the PyTorch framework. It facilitates the computation of gradients for tensor operations, which is crucial for optimizing machine learning models using algorithms like gradient descent."
      ],
      "metadata": {
        "id": "Ba04tCseCc5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors.\n",
        "\n",
        "Training a NN happens in two steps:\n",
        "\n",
        "* **Forward Propagation:** In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n",
        "\n",
        "* **Backward Propagation:** In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent."
      ],
      "metadata": {
        "id": "8fBBKkGTslGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "DNeAfvzOCmUO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_iNkskIuAmdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6175412e-7c30-49f5-fb74-4a36c1143ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(9., grad_fn=<PowBackward0>)\n",
            "tensor(6.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2\n",
        "print(x)\n",
        "print(y)\n",
        "# To calculate differentiation\n",
        "y.backward()\n",
        "# To show differentiation\n",
        "print(x.grad) # dy/dx = 2*x  # 6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def dz_dx(x):\n",
        "  return 2*x*math.cos(x**2)\n",
        "print(dz_dx(3))\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "print(x)\n",
        "y = x**2\n",
        "print(y)\n",
        "z = torch.sin(y)\n",
        "print(z)\n",
        "# To calculate differentiation\n",
        "z.backward()\n",
        "# To show differentiation\n",
        "print(x.grad)\n"
      ],
      "metadata": {
        "id": "2jWmqPXYCiF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9e1c86-f91a-461b-fff3-4ab74b7c19e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-5.466781571308061\n",
            "tensor(3., requires_grad=True)\n",
            "tensor(9., grad_fn=<PowBackward0>)\n",
            "tensor(0.4121, grad_fn=<SinBackward0>)\n",
            "tensor(-5.4668)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing Grad\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2\n",
        "z = torch.sin(y)\n",
        "z.backward() # Calculate the gradient before accessing x.grad\n",
        "print(x.grad)\n",
        "x.grad.zero_()\n",
        "print(x.grad)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "Nzsa2nufCiCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8675e53c-65bc-4aa6-9214-2c411c0af8c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-5.4668)\n",
            "tensor(0.)\n",
            "tensor(3., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing Grad\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "print(x)\n",
        "y = x**2\n",
        "z = torch.sin(y)\n",
        "z.backward() # Calculate the gradient before accessing x.grad\n",
        "print(x.grad)\n",
        "x.requires_grad_(False)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnN__Nks9oS8",
        "outputId": "2f00184f-3ef4-41ac-fd56-9eb768837f56"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(-5.4668)\n",
            "tensor(3.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# disable gradient tracking\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x**2\n",
        "print(y)\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "##\n",
        "# Options 1: requires_grad_(False)\n",
        "# Options 2: detach()\n",
        "# Options 3: troch.no_grad()\n",
        "x.requires_grad_(False)\n",
        "print(x.grad)\n",
        "y = x**2\n",
        "print(y)\n",
        "try :\n",
        "  y.backward()\n",
        "  print(x.grad)\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ],
      "metadata": {
        "id": "JMxQL_Y5Ch_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45482222-2f76-4996-a709-ef0480b1f65c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9., grad_fn=<PowBackward0>)\n",
            "tensor(6.)\n",
            "tensor(6.)\n",
            "tensor(9.)\n",
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "z = x.detach()\n",
        "print(z)\n",
        "y = z**2\n",
        "try:\n",
        "  y.backward()\n",
        "  print(x.grad)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "p4vExyFMCh6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1554b7-516a-4a0d-dfb6-206d5770c84c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n",
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "print(x)\n",
        "with torch.no_grad():\n",
        "  y = x**2\n",
        "  print(y)\n",
        "try:\n",
        "  y.backward()\n",
        "  print(x.grad)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "RyBAJhHbCh36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b217e28-6848-4b57-a35d-74393459ba64"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(9.)\n",
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO ENABEL YOUR GPU\n",
        "print(torch.cuda.is_available())\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "-LogGR8vCh1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1193d4c-d02a-4910-d682-1401c72a883e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# creating a new tensor in GPU\n",
        "x = torch.tensor([3.0,7.0], device=device, requires_grad=True)\n",
        "print(x)\n",
        "y = x**2\n",
        "print(y)\n",
        "z = torch.sin(y)\n",
        "print(z)\n",
        "w = torch.exp(z)\n",
        "print(w)\n",
        "v = 1/w\n",
        "print(v)\n",
        "# To calculate differentiation\n",
        "t= v.sum()\n",
        "t.backward()\n",
        "# To show differentiation\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GptVLeQN-L6j",
        "outputId": "3f8e570b-8b97-4d6e-f8b4-79a55cd02f77"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 7.], device='cuda:0', requires_grad=True)\n",
            "tensor([ 9., 49.], device='cuda:0', grad_fn=<PowBackward0>)\n",
            "tensor([ 0.4121, -0.9538], device='cuda:0', grad_fn=<SinBackward0>)\n",
            "tensor([1.5100, 0.3853], device='cuda:0', grad_fn=<ExpBackward0>)\n",
            "tensor([0.6622, 2.5954], device='cuda:0', grad_fn=<MulBackward0>)\n",
            "tensor([  3.6204, -10.9223], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Computational Graph**\n",
        "Conceptually, autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in a **directed acyclic graph (DAG)** consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
        "\n",
        "In a forward pass, autograd does two things simultaneously:\n",
        "\n",
        "* run the requested operation to compute a resulting tensor, and\n",
        "* maintain the operation’s gradient function in the DAG.\n",
        "\n",
        "The backward pass kicks off when `.backward()` is called on the DAG root. `autograd` then:\n",
        "\n",
        "* computes the gradients from each `.grad_fn`,\n",
        "* accumulates them in the respective tensor’s `.grad` attribute, and\n",
        "* using the chain rule, propagates all the way to the leaf tensors."
      ],
      "metadata": {
        "id": "4xBGlgy6BPWm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3uYO4Iz1Aeuv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7PHsJObsAgBT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBSCvfq9Ahev"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}